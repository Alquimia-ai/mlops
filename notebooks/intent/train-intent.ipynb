{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "746d98ff-75fc-4177-a596-46ddb8ecc558",
   "metadata": {},
   "source": [
    "# Fine tune bert model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e83f33-ca47-4c79-9e89-c97842753f1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load dataset from downloads 沒･沒･"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92c12f0a-4b1a-4402-8b4a-75f873a2e14b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.28.0\n",
      "  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m329.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting evaluate\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m257.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m331.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/app-root/lib/python3.9/site-packages (from transformers==4.28.0) (1.24.4)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m273.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from transformers==4.28.0) (2.31.0)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from transformers==4.28.0) (3.12.4)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m186.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/app-root/lib/python3.9/site-packages (from transformers==4.28.0) (4.66.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.10.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m773.3/773.3 kB\u001b[0m \u001b[31m343.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/app-root/lib/python3.9/site-packages (from transformers==4.28.0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.9/site-packages (from transformers==4.28.0) (6.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/app-root/lib/python3.9/site-packages (from datasets) (0.3.7)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m193.8/193.8 kB\u001b[0m \u001b[31m266.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/app-root/lib/python3.9/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /opt/app-root/lib/python3.9/site-packages (from datasets) (2023.10.0)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.15-py39-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m300.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-14.0.1-cp39-cp39-manylinux_2_28_x86_64.whl (38.0 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m213.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /opt/app-root/lib/python3.9/site-packages (from datasets) (3.8.6)\n",
      "Collecting pyarrow-hotfix\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: psutil in /opt/app-root/lib/python3.9/site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/app-root/lib/python3.9/site-packages (from accelerate) (2.0.1+cu118)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (3.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.8.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers==4.28.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers==4.28.0) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers==4.28.0) (3.4)\n",
      "Requirement already satisfied: jinja2 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: networkx in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.2)\n",
      "Requirement already satisfied: sympy in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: lit in /opt/app-root/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (17.0.3)\n",
      "Requirement already satisfied: cmake in /opt/app-root/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/app-root/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Installing collected packages: tokenizers, xxhash, regex, pyarrow-hotfix, pyarrow, multiprocess, responses, huggingface-hub, transformers, datasets, evaluate, accelerate\n",
      "Successfully installed accelerate-0.24.1 datasets-2.15.0 evaluate-0.4.1 huggingface-hub-0.19.4 multiprocess-0.70.15 pyarrow-14.0.1 pyarrow-hotfix-0.6 regex-2023.10.3 responses-0.18.0 tokenizers-0.13.3 transformers-4.28.0 xxhash-3.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.28.0 datasets evaluate accelerate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c7ff2a9-899b-45f7-8c29-dcca5549a5b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_id</th>\n",
       "      <th>annotator</th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>question</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1120</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-11-08T20:41:06.680849Z</td>\n",
       "      <td>1165</td>\n",
       "      <td>2.084</td>\n",
       "      <td>I ordered two pairs of skis, 173cm, and they s...</td>\n",
       "      <td>postSale</td>\n",
       "      <td>2023-11-08T20:41:06.680863Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>283</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-10-30T13:20:25.196662Z</td>\n",
       "      <td>287</td>\n",
       "      <td>2.081</td>\n",
       "      <td>What does \"lithotripsy\" mean?</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>2023-10-30T13:20:25.196675Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1295</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-11-10T15:55:32.460520Z</td>\n",
       "      <td>1340</td>\n",
       "      <td>1.637</td>\n",
       "      <td>I bought a summer skirt for my daughter in a s...</td>\n",
       "      <td>postSale</td>\n",
       "      <td>2023-11-10T15:55:32.460532Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1085</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-11-03T23:19:37.431473Z</td>\n",
       "      <td>1130</td>\n",
       "      <td>1.527</td>\n",
       "      <td>I appreciated the user-friendly payment platfo...</td>\n",
       "      <td>feedback</td>\n",
       "      <td>2023-11-03T23:19:37.431494Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1307</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-11-10T15:57:10.625237Z</td>\n",
       "      <td>1352</td>\n",
       "      <td>2.595</td>\n",
       "      <td>I acquired a Rowdy hoodie in size small, but I...</td>\n",
       "      <td>postSale</td>\n",
       "      <td>2023-11-10T15:57:10.625259Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   annotation_id  annotator                   created_at    id  lead_time  \\\n",
       "0           1120          1  2023-11-08T20:41:06.680849Z  1165      2.084   \n",
       "1            283          1  2023-10-30T13:20:25.196662Z   287      2.081   \n",
       "2           1295          1  2023-11-10T15:55:32.460520Z  1340      1.637   \n",
       "3           1085          1  2023-11-03T23:19:37.431473Z  1130      1.527   \n",
       "4           1307          1  2023-11-10T15:57:10.625237Z  1352      2.595   \n",
       "\n",
       "                                            question   sentiment  \\\n",
       "0  I ordered two pairs of skis, 173cm, and they s...    postSale   \n",
       "1                      What does \"lithotripsy\" mean?  irrelevant   \n",
       "2  I bought a summer skirt for my daughter in a s...    postSale   \n",
       "3  I appreciated the user-friendly payment platfo...    feedback   \n",
       "4  I acquired a Rowdy hoodie in size small, but I...    postSale   \n",
       "\n",
       "                    updated_at  \n",
       "0  2023-11-08T20:41:06.680863Z  \n",
       "1  2023-10-30T13:20:25.196675Z  \n",
       "2  2023-11-10T15:55:32.460532Z  \n",
       "3  2023-11-03T23:19:37.431494Z  \n",
       "4  2023-11-10T15:57:10.625259Z  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "labeled_dataset = \"labeled_dataset.csv\"\n",
    "\n",
    "# Assuming the file is in the current working directory\n",
    "df = pd.read_csv(labeled_dataset)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c714855b-8eae-4822-ab2d-59f46c748958",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_size = 0.2 ## Define the testing size for metrics\n",
    "number_of_labels = os.environ.get('number_labels', 6)\n",
    "label_column_name = 'sentiment'\n",
    "text_column_name = os.environ.get('prompt_column', 'question')\n",
    "model_name = \"intent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcc7068f-7b68-4d34-83c1-3060e7e94221",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of labels to be showed are: 6 with a label colum name: sentiment and a prompt column name: question (The test size is 0.2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of labels to be showed are: {number_of_labels} with a label colum name: {label_column_name} and a prompt column name: {text_column_name} (The test size is {test_size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6c6b30-3c04-4693-bbb6-f4249e051824",
   "metadata": {},
   "source": [
    "## Define mappings \n",
    "\n",
    "Here you have to define a map so the model can be properly trained lets see an example\n",
    "\n",
    "```python\n",
    "category_to_label = {\n",
    "    'availability': 0,\n",
    "    'irrelevant': 1,\n",
    "    'post sale': 2,\n",
    "    'invoice':3,\n",
    "    'service':4,\n",
    "    'pricing':5,\n",
    "    'general':6,\n",
    "    'cancelation policy':7,\n",
    "    'cancel reservation':8\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adfc3d0d-7d5e-418c-9b54-c0609aa38172",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### For now the map is being defined within the notebook\n",
    "\n",
    "category_to_label={\n",
    " 'inventory': 0,\n",
    " 'checkout': 1,\n",
    " 'irrelevant': 2,\n",
    " 'conversational': 3,\n",
    " 'feedback': 4,\n",
    " 'postSale': 5\n",
    "}\n",
    "\n",
    "# Add the new 'label' column to the dataframe by mapping values from the 'category' column\n",
    "df['label'] = df[label_column_name].replace(category_to_label)\n",
    "df = df.drop('annotation_id',axis=1)\n",
    "df = df.drop('annotator',axis=1)\n",
    "df = df.drop('created_at',axis=1)\n",
    "df = df.drop('id',axis=1)\n",
    "df = df.drop('lead_time',axis=1)\n",
    "df = df.drop('updated_at',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7715447-cd95-419f-a985-bb490457f205",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inventory': 0,\n",
       " 'checkout': 1,\n",
       " 'irrelevant': 2,\n",
       " 'conversational': 3,\n",
       " 'feedback': 4,\n",
       " 'postSale': 5}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfab0f2b-d9c5-4474-b1cc-5cb543217eff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I ordered two pairs of skis, 173cm, and they s...</td>\n",
       "      <td>postSale</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What does \"lithotripsy\" mean?</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I bought a summer skirt for my daughter in a s...</td>\n",
       "      <td>postSale</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question   sentiment  label\n",
       "0  I ordered two pairs of skis, 173cm, and they s...    postSale      5\n",
       "1                      What does \"lithotripsy\" mean?  irrelevant      2\n",
       "2  I bought a summer skirt for my daughter in a s...    postSale      5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d44bcdc3-c6a4-4067-98bc-79f8df3a3012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2812aeee-aa35-4ffc-ba6e-1466296c7fe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "test_dataset = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6b8f309-1946-48c2-a16c-0965ab400919",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912ea2640d7d4b9ca939fa2b1fc3984e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2676371c9b4596a1f39bc2b38825f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd8b114a9324b5d8b08450f54416afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b6f237c9de4b6e9d2fb1bc1f25bbbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "base_model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8c52911-84cf-4c1f-9370-7d9a591e4f66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[text_column_name], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0704fc0d-5f1d-48d1-afd9-fafbd2740e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203ca076ba5c40e3ae2fa2b8239a0344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/386 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae337a8101142a598795565b5d0db47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/97 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f1409db-e337-4414-a23d-c03ecd4d30b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8839e8db2f843daa0a98818301386cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=number_of_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcc1853b-bba9-4d4b-bb04-ed50abe1808f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "353bdc9b-f4ad-45d2-980f-ea3c425ee679",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ea02b4f59d4eceac7a436da4e91726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "  logits, labels = eval_pred\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "  return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39caf7-449e-4187-901a-0b435d3dbe34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "352b826f-e3fa-4d6d-9cb1-a1f205585698",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    hub_model_id=model_name,\n",
    "    output_dir=\"./output\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_dataset=tokenized_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cd8554c-e0de-4f83-9802-c5c0f35e41fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='245' max='245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [245/245 02:46, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.654200</td>\n",
       "      <td>0.417160</td>\n",
       "      <td>0.917526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.190700</td>\n",
       "      <td>0.316821</td>\n",
       "      <td>0.917526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.143200</td>\n",
       "      <td>0.248782</td>\n",
       "      <td>0.938144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.068600</td>\n",
       "      <td>0.230845</td>\n",
       "      <td>0.948454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.067900</td>\n",
       "      <td>0.255307</td>\n",
       "      <td>0.958763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=245, training_loss=0.224959135055542, metrics={'train_runtime': 167.5709, 'train_samples_per_second': 11.518, 'train_steps_per_second': 1.462, 'total_flos': 16476807985920.0, 'train_loss': 0.224959135055542, 'epoch': 5.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Fine tune model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5f1dc20-5fcd-4f8d-9cc7-e39184bda1df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Save pytorch \n",
    "trainer.save_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f537b1d6-2e21-4e44-ad88-1b96434045c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx onnxoptimizer -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d38ae700-bde1-48f0-86d1-6dfc01d4286e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:223: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import onnx\n",
    "import onnxoptimizer\n",
    "\n",
    "# Load the fine-tuned DistilBERT model and tokenizer\n",
    "model_checkpoint = model_name\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Create dummy input data for inference\n",
    "text = \"Do you have red t shirts?\"\n",
    "input_data = tokenizer.encode_plus(text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "# Export the PyTorch model to ONNX\n",
    "onnx_filename = f\"{model_name}.onnx\"\n",
    "dummy_input = input_data[\"input_ids\"]\n",
    "torch.onnx.export(model, (dummy_input,), onnx_filename, input_names=['input_ids'], output_names=['logits'])\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(onnx_filename)\n",
    "\n",
    "# Optimize the ONNX model using onnxoptimizer\n",
    "optimized_model = onnxoptimizer.optimize(onnx_model)\n",
    "\n",
    "# Save the optimized ONNX model using file handling\n",
    "optimized_onnx_filename = \"optimized_model.onnx\"\n",
    "with open(optimized_onnx_filename, \"wb\") as f:\n",
    "    f.write(optimized_model.SerializeToString())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17b18462-91a8-45c5-be83-c1939174e9ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Load the bucket and file path\n",
    "bucket_name =  os.environ['bucket_name']\n",
    "model_path =  os.environ.get('model_base_dir', \"ecommerce-medusa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6699473a-0269-4012-a95c-5a783d8ec506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## AWS_ACCESS_KEY_ID & AWS_SECRET_ACCESS_KEY should be set as Env variables\n",
    "key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea6b6efc-acc7-44e2-b903-f7c4edf7c251",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "now_date = datetime.now()\n",
    "s3_client = boto3.client('s3', aws_access_key_id=key_id, aws_secret_access_key=secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1b5a720-4620-486a-ac34-03d7783e7225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Upload file to S3\n",
    "s3_client.upload_file(optimized_onnx_filename, bucket_name, f\"{model_path}/bins/{onnx_filename}\")\n",
    "s3_client.upload_file(labeled_dataset, bucket_name,  f\"{model_path}/datasets/dataset-{now_date.isoformat()}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "943c5e2c-cd92-4df0-8e82-a1587f5ba835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Delete directory in Jupyter Notebook\n",
    "import shutil\n",
    "\n",
    "# Remove the local model directory\n",
    "shutil.rmtree(model_name)\n",
    "os.remove(optimized_onnx_filename)\n",
    "os.remove(onnx_filename)\n",
    "os.remove(labeled_dataset)\n",
    "shutil.rmtree(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32201a27-96f6-440e-9ce5-ac501325e744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
